Big data is a broad term for data sets so large or complex that traditional data processing applications are inadequate.
Challenges include analysis, capture, data curation, search, sharing, storage, transfer, visualization, querying and information privacy.
The term often refers simply to the use of predictive analytics or certain other advanced methods to extract value from data, and seldom to a particular size of data set.
Accuracy in big data may lead to more confident decision making, and better decisions can result in greater operational efficiency, cost reduction and reduced risk.
Analysis of data sets can find new correlations to "spot business trends, prevent diseases, combat crime and so on."
Scientists, business executives, practitioners of medicine, advertising and governments alike regularly meet difficulties with large data sets in areas including Internet search, finance and business informatics.
Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology and environmental research.
Data sets are growing rapidly in part because they are increasingly gathered by cheap and numerous information-sensing mobile devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks.
The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.5×1018) of data are created.
One question for large enterprises is determining who should own big data initiatives that affect the entire organization.
Relational database management systems and desktop statistics and visualization packages often have difficulty handling big data.
The work instead requires "massively parallel software running on tens, hundreds, or even thousands of servers".
What is considered "big data" varies depending on the capabilities of the users and their tools, and expanding capabilities make big data a moving target.
"For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options.
For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration."
Big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time.
Big data "size" is a constantly moving target, as of 2012 ranging from a few dozen terabytes to many petabytes of data.
Big data requires a set of techniques and technologies with new forms of integration to reveal insights from datasets that are diverse, complex, and of a massive scale.
In a 2001 research report and related lectures, META Group (now Gartner) analyst Doug Laney defined data growth challenges and opportunities as being three-dimensional, i.e. increasing volume (amount of data), velocity (speed of data in and out), and variety (range of data types and sources).
Gartner, and now much of the industry, continue to use this "3Vs" model for describing big data.
In 2012, Gartner updated its definition as follows: "Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization."
Gartner's definition of the 3Vs is still widely used, and in agreement with a consensual definition that states that "Big Data represents the Information assets characterized by such a High Volume, Velocity and Variety to require specific Technology and Analytical Methods for its transformation into Value".
Additionally, a new V "Veracity" is added by some organizations to describe it, revisionism challenged by some industry authorities.
The 3Vs have been expanded to other complementary characteristics of big data:
Volume: big data doesn't sample; it just observes and tracks what happens
Velocity: big data is often available in real-time
Variety: big data draws from text, images, audio, video; plus it completes missing pieces through data fusion
Machine Learning: big data often doesn't ask why and simply detects patterns
Digital footprint: big data is often a cost-free byproduct of digital interaction
Big data requires exceptional technologies to efficiently process large quantities of data within tolerable elapsed times.
A 2011 McKinsey report suggests suitable technologies include A/B testing, crowdsourcing, data fusion and integration, genetic algorithms, machine learning, natural language processing, signal processing, simulation, time series analysis and visualisation.
Multidimensional big data can also be represented as tensors, which can be more efficiently handled by tensor-based computation, such as multilinear subspace learning.
Additional technologies being applied to big data include massively parallel-processing (MPP) databases, search-based applications, data mining, distributed file systems, distributed databases, cloud-based infrastructure (applications, storage and computing resources) and the Internet.
Some but not all MPP relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the RDBMS.
DARPA's Topological Data Analysis program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called Ayasdi.
The practitioners of big data analytics processes are generally hostile to slower shared storage, preferring direct-attached storage (DAS) in its various forms from solid state drive (Ssd) to high capacity SATA disk buried inside parallel processing nodes.
The perception of shared storage architectures—Storage area network (SAN) and Network-attached storage (NAS) —is that they are relatively slow, complex, and expensive.
These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.
Real or near-real time information delivery is one of the defining characteristics of big data analytics.
Latency is therefore avoided whenever and wherever possible.
Data in memory is good—data on spinning disk at the other end of a FC SAN connection is not.
The cost of a SAN at the scale needed for analytics applications is very much higher than other storage techniques.
There are advantages as well as disadvantages to shared storage in big data analytics, but big data analytics practitioners as of 2011 did not favour it.[50]
Big data has increased the demand of information management specialists in that Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP and Dell have spent more than $15 billion on software firms specializing in data management and analytics.
In 2010, this industry was worth more than $100 billion and was growing at almost 10 percent a year: about twice as fast as the software business as a whole.
Developed economies increasingly use data-intensive technologies.
There are 4.6 billion mobile-phone subscriptions worldwide, and between 1 billion and 2 billion people accessing the internet.
Between 1990 and 2005, more than 1 billion people worldwide entered the middle class, which means more people become more literate, which in turn leads to information growth.
The world's effective capacity to exchange information through telecommunication networks was 281 petabytes in 1986, 471 petabytes in 1993, 2.2 exabytes in 2000, 65 exabytes in 2007 and predictions put the amount of internet traffic at 667 exabytes annually by 2014.
According to one estimate, one third of the globally stored information is in the form of alphanumeric text and still image data,
which is the format most useful for most big data applications.
This also shows the potential of yet unused data (i.e. in the form of video and audio content).
While many vendors offer off-the-shelf solutions for Big Data, experts recommend the development of in-house solutions custom-tailored to solve the company's problem at hand if the company has sufficient technical capabilities.
Machine learning tasks are typically classified into three broad categories, depending on the nature of the learning "signal" or "feedback" available to a learning system.
Supervised learning: The computer is presented with example inputs and their desired outputs, given by a "teacher", and the goal is to learn a general rule that maps inputs to outputs.
Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input.
Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).
Reinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle), without a teacher explicitly telling it whether it has come close to its goal.
Another example is learning to play a game by playing against an opponent.
Between supervised and unsupervised learning is semi-supervised learning, where the teacher gives an incomplete training signal: a training set with some (often many) of the target outputs missing.
Transduction is a special case of this principle where the entire set of problem instances is known at learning time, except that part of the targets are missing.
A support vector machine is a classifier that divides its input space into two regions, separated by a linear boundary.
Here, it has learned to distinguish black and white circles.
Among other categories of machine learning problems, learning to learn learns its own inductive bias based on previous experience.
Developmental learning, elaborated for robot learning, generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers, and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation.
Another categorization of machine learning tasks arises when one considers the desired output of a machine-learned system:
In classification, inputs are divided into two or more classes, and the learner must produce a model that assigns unseen inputs to one (or multi-label classification) or more of these classes.
This is typically tackled in a supervised way.
Spam filtering is an example of classification, where the inputs are email (or other) messages and the classes are "spam" and "not spam".
In regression, also a supervised problem, the outputs are continuous rather than discrete.
In clustering, a set of inputs is to be divided into groups.
Unlike in classification, the groups are not known beforehand, making this typically an unsupervised task.
Density estimation finds the distribution of inputs in some space.
Dimensionality reduction simplifies inputs by mapping them into a lower-dimensional space.
Topic modeling is a related problem, where a program is given a list of human language documents and is tasked to find out which documents cover similar topics.
A core objective of a learner is to generalize from its experience.
Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set.
The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.
The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory.
Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms.
Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.
In addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning.
In computational learning theory, a computation is considered feasible if it can be done in polynomial time.
There are two kinds of time complexity results.
Positive results show that a certain class of functions can be learned in polynomial time.
Negative results show that certain classes cannot be learned in polynomial time.
There are many similarities between machine learning theory and statistical inference, although they use different terms.
An artificial neural network (ANN) learning algorithm, usually called "neural network" (NN), is a learning algorithm that is inspired by the structure and functional aspects of biological neural networks.
Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation.
Modern neural networks are non-linear statistical data modeling tools.
They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.
Several learning algorithms, mostly unsupervised learning algorithms, aim at discovering better representations of the inputs provided during training.
Classical examples include principal components analysis and cluster analysis.
Representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing to reconstruct the inputs coming from the unknown data generating distribution, while not being necessarily faithful for configurations that are implausible under that distribution.
Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional.
Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse (has many zeros).
Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into (high-dimensional) vectors.
Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features.
It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.
Despite popular belief, a reliance on a single food which composes the majority of a diet is indicative of poor eating habits.
An individual on such a diet may be prone to deficiency and most certainly will not be fulfilling the Recommended Nutrient Intake.
While plants, vegetables, and fruits are known to help reduce the incidence of chronic disease, the benefits on health posed by plant-based foods, as well as the percentage on which a diet needs to be plant-based in order to have health benefits, is unknown.
Nevertheless, plant-based food diets in society and between nutritionist circles are linked to health and longevity, as well as contributing to lowering cholesterol, weight loss, and, in some cases, stress reduction.
Although a number of preconceptions of a healthy diet center around plant-based foods, the majority of assumptions about foods which are usually thought of as "bad" foods are usually correct, apart from the assumption that there are "bad" foods; many people associate dishes such as Full English cooked Breakfast and Bacon Sandwiches as foods which, if eaten regularly, can contribute to cholesterol, fat, and heart problems.
A healthy diet is usually defined as a diet in which nutrient intake is maintained, and cholesterol, salt, sugar, and fat are reduced.
The idea of a healthy diet is something used by a government to ensure that people are well "protected" against common illnesses and conditions which stem from poor diet.
This could include headaches, lessened sexual drive, heart disease, alcohol poisoning, or obesity.
A healthy diet is a way of eating that that reduces risk for complications such as heart disease and stroke.
Healthy eating includes eating a wide variety of foods including: vegetables, whole grains, fruits, non-fat dairy products, beans, lean meats, poultry, fish
The definition of a healthy diet is sometimes also thought of as a diet which will combat or prevent illness.
Although the majority of people would support this definition, few know why, other than because "bad" foods are not consumed.
People with healthy diets are less likely to succumb to common minor illnesses, such as lesser forms of Influenza, mainly because consumption of a healthy diet would provide ample nutrients and energy for the body, so as to help stave off such illnesses.
Similarly, the healthy diet can also be used this way to aid the body during illness.
The myth of "feed a cold, starve a fever" is a common misconception among the public, particularly in the United Kingdom.
This is a myth in every sense of the word because providing the body with nutrients during illness is actually beneficial - nutrient and energy stores would be replenished, allowing for more energy to be used by the body to combat illness.
Cholesterol is a steroid, a lipid, and an alcohol, found in the cell membranes of all body tissues, and transported in the blood plasma of all animals.
Most cholesterol is not dietary in origin, it is synthesized internally.
Cholesterol is present in higher concentrations in tissues which either produce more or have more densely-packed membranes, for example, the liver, spinal cord, brain and atheroma.
Cholesterol plays a central role in many biochemical processes, but is best known for the association of cardiovascular disease with various lipoprotein cholesterol transport patterns in the blood.
The name originates from the Greek chole- (bile) and stereos (solid), as researchers first identified cholesterol (C27H45OH) in solid form in gallstones.
The selective toxicity of vitamin C for cancer cells has been demonstrated repeatedly in cell culture studies.
The Proceedings of the National Academy of Sciences recently published a paper demonstrating vitamin C killing cancer cells.
As of 2005, some physicians have called for a more careful reassessment of vitamin C, especially intravenous vitamin C, in cancer treatment.
With two colleagues, Pauling founded the Institute of Orthomolecular Medicine in Menlo Park, California, in 1973, which was soon renamed the Linus Pauling Institute of Science and Medicine.
Pauling directed research on vitamin C, but also continued his theoretical work in chemistry and physics until his death in 1994.
In his last years, he became especially interested in the possible role of vitamin C in preventing atherosclerosis and published three case reports on the use of lysine and vitamin C to relieve angina pectoris.
In 1996, the Linus Pauling Institute moved from Palo Alto, California, to Corvallis, Oregon, to become part of Oregon State University, where it continues to conduct research on micronutrients, phytochemicals (chemicals from plants), and other constituents of the diet in preventing and treating disease.
Stress management refers to the wide spectrum of techniques and psychotherapies aimed at controlling a person's levels of stress, especially chronic stress, usually for the purpose of improving everyday functioning.
In this context, the term 'stress' refers only to a stress with significant negative consequences, or distress in the terminology advocated by Hans Selye, rather than what he calls eustress, a stress whose consequences are helpful or otherwise positive.
Stress produces numerous physical and mental symptoms which vary according to each individual's situational factors.
These can include physical health decline as well as depression.
The process of stress management is named as one of the keys to a happy and successful life in modern society.
Although life provides numerous demands that can prove difficult to handle, stress management provides a number of ways to manage anxiety and maintain overall well-being.
Despite stress often being thought of as a subjective experience, levels of stress are readily measurable, using various physiological tests, similar to those used in polygraphs.
Many practical stress management techniques are available, some for use by health professionals and others, for self-help, which may help an individual reduce their levels of stress, provide positive feelings of control over one's life and promote general well-being.
Evaluating the effectiveness of various stress management techniques can be difficult, as limited research currently exists.
Consequently, the amount and quality of evidence for the various techniques varies widely.
Some are accepted as effective treatments for use in psychotherapy, whilst others with less evidence favoring them are considered alternative therapies.
Many professional organisations exist to promote and provide training in conventional or alternative therapies.
There are several models of stress management, each with distinctive explanations of mechanisms for controlling stress.
Much more research is necessary to provide a better understanding of which mechanisms actually operate and are effective in practice.
Unintentional weight loss may result from loss of body fats, loss of body fluids, muscle atrophy, or even a combination of these.
It is generally regarded as a medical problem when at least 10% of a person's body weight has been lost in six months or 5% in the last month.
Another criterion used for assessing weight that is too low is the body mass index (BMI).
However, even lesser amounts of weight loss can be a cause for serious concern in a frail elderly person.
Unintentional weight loss can occur because of an inadequately nutritious diet relative to a person's energy needs (generally called malnutrition).
Disease processes, changes in metabolism, hormonal changes, medications or other treatments, disease- or treatment-related dietary changes, or reduced appetite associated with a disease or treatment can also cause unintentional weight loss.
Poor nutrient utilization can lead to weight loss, and can be caused by fistulae in the gastrointestinal tract, diarrhea, drug-nutrient interaction, enzyme depletion and muscle atrophy.
Continuing weight loss may deteriorate into wasting, a vaguely defined condition called cachexia.
Cachexia differs from starvation in part because it involves a systemic inflammatory response.
It is associated with poorer outcomes.
In the advanced stages of progressive disease, metabolism can change so that they lose weight even when they are getting what is normally regarded as adequate nutrition and the body cannot compensate.
This leads to a condition called anorexia cachexia syndrome (ACS) and additional nutrition or supplementation is unlikely to help.
Symptoms of weight loss from ACS include severe weight loss from muscle rather than body fat, loss of appetite and feeling full after eating small amounts, nausea, anemia, weakness and fatigue.
Serious weight loss may reduce quality of life, impair treatment effectiveness or recovery, worsen disease processes and be a risk factor for high mortality rates.
Medical treatment can directly or indirectly cause weight loss, impairing treatment effectiveness and recovery that can lead to further weight loss in a vicious cycle.
Many patients will be in pain and have a loss of appetite after surgery.
Part of the body's response to surgery is to direct energy to wound healing, which increases the body's overall energy requirements.
Surgery affects nutritional status indirectly, particularly during the recovery period, as it can interfere with wound healing and other aspects of recovery.
Surgery directly affects nutritional status if a procedure permanently alters the digestive system.
Enteral nutrition (tube feeding) is often needed.
However a policy of 'nil by mouth' for all gastrointestinal surgery has not been shown to benefit, with some suggestion it might hinder recovery.
Early post-operative nutrition is a part of Enhanced Recovery After Surgery protocols.
These protocols also include carbohydrate loading in the 24 hours before surgery, but earlier nutritional interventions have not been shown to have a significant impact.
Some medications can cause weight loss, while others can cause weight gain.
Social conditions such as poverty, social isolation and inability to get or prepare preferred foods can cause unintentional weight loss, and this may be particularly common in older people.
Nutrient intake can also be affected by culture, family and belief systems.
Ill-fitting dentures and other dental or oral health problems can also affect adequacy of nutrition.
Loss of hope, status or social contact and spiritual distress can cause depression, which may be associated with reduced nutrition, as can fatigue.
Intentional weight loss is the loss of total body mass as a result of efforts to improve fitness and health, or to change appearance through slimming.
Weight loss in individuals who are overweight or obese can reduce health risks, increase fitness, and may delay the onset of diabetes.
It could reduce pain and increase movement in people with osteoarthritis of the knee.
Weight loss can lead to a reduction in hypertension (high blood pressure), however whether this reduces hypertension-related harm is unclear.
Weight loss occurs when the body is expending more energy in work and metabolism than it is absorbing from food or other nutrients.
It will then use stored reserves from fat or muscle, gradually leading to weight loss.
For athletes seeking to improve performance or to meet required weight classification for participation in a sport, it is not uncommon to seek additional weight loss even if they are already at their ideal body weight.
Others may be driven to lose weight to achieve an appearance they consider more attractive.
Being underweight is associated with health risks such as difficulty fighting off infection, osteoporosis, decreased muscle strength, trouble regulating body temperature and even increased risk of death.
Low-calorie diets are also referred to as balanced percentage diets.
Due to their minimal detrimental effects, these types of diets are most commonly recommended by nutritionists.
In addition to restricting calorie intake, a balanced diet also regulates macronutrient consumption.
From the total number of allotted daily calories, it is recommended that 55% should come from carbohydrates, 15% from protein, and 30% from fats with no more than 10% of total fat coming from saturated forms.
For instance, a recommended 1,200 calorie diet would supply about 660 calories from carbohydrates, 180 from protein, and 360 from fat.
Some studies suggest that increased consumption of protein can help ease hunger pangs associated with reduced caloric intake by increasing the feeling of satiety.
Calorie restriction in this way has many long-term benefits. After reaching the desired body weight, the calories consumed per day may be increased gradually, without exceeding 2,000 net (i.e. derived by subtracting calories burned by physical activity from calories consumed).
Combined with increased physical activity, low-calorie diets are thought to be most effective long-term, unlike crash diets, which can achieve short-term results, at best.
Physical activity could greatly enhance the efficiency of a diet.
The healthiest weight loss regimen, therefore, is one that consists of a balanced diet and moderate physical activity.
The least intrusive weight loss methods, and those most often recommended, are adjustments to eating patterns and increased physical activity, generally in the form of exercise.
The World Health Organization recommended that people combine a reduction of processed foods high in saturated fats, sugar and salt and caloric content of the diet with an increase in physical activity.
An increase in fiber intake is also recommended for regulating bowel movements.
Other methods of weight loss include use of drugs and supplements that decrease appetite, block fat absorption, or reduce stomach volume.
Bariatric surgery may be indicated in cases of severe obesity.
Two common bariatric surgical procedures are gastric bypass and gastric banding.
Both can be effective at limiting the intake of food energy by reducing the size of the stomach, but as with any surgical procedure both come with their own risks that should be considered in consultation with a physician.
Dietary supplements, though widely used, are not considered a healthy option for weight loss.
Many are available, but very few are effective in the long term.
Virtual gastric band uses hypnosis to make the brain think the stomach is smaller than it really is and hence lower the amount of food ingested.
This brings as a consequence weight reduction.
This method is complemented with psychological treatment for anxiety management and with hypnopedia.
Research has been conducted into the use of hypnosis as a weight management alternative.
In 1996 a study found that cognitive-behavioral therapy (CBT) was more effective for weight reduction if reinforced with hypnosis.
Acceptance and Commitment Therapy ACT, a mindfulness approach to weight loss, has also in the last few years been demonstrating its usefulness.
Symptoms of parasites may not always be obvious.
However, such symptoms may mimic anemia or a hormone deficiency.
Some of the symptoms caused by several worm infestation can include itching affecting the anus or the vaginal area, abdominal pain, weight loss, increased appetite, bowel obstructions, diarrhea, and vomiting eventually leading to dehydration, sleeping problems, worms present in the vomit or stools, anemia, aching muscles or joints, general malaise, allergies, fatigue, nervousness.
Symptoms may also be confused with pneumonia or food poisoning.
The effects caused by parasitic diseases range from mild discomfort to death.
The nematode parasites Necator americanus and Ancylostoma duodenale cause human hookworm infection, which leads to anaemia and protein malnutrition.
This infection affects approximately 740 million people in the developing countries, including children and adults, of the tropics specifically in poor rural areas located in sub-Saharan Africa, Latin America, South-East Asia and China.
Chronic hookworm in children leads to impaired physical and intellectual development, school performance and attendance are reduced.
Pregnant women affected by a hookworm infection can also develop aneamia, which results in negative outcomes both for the mother and the infant.
Some of them are: low birth weight, impaired milk production, as well as increased risk of death for the mother and the baby.
Issuer services help companies from around the world to join the London equity market in order to gain access to capital.
The LSE allows companies to raise money, increase their profile and obtain a market valuation through a variety of routes, thus following the firms throughout the whole IPO process.
The London Stock Exchange runs several markets for listing, giving an opportunity for different sized companies to list.
International companies can list a number of products in London including shares, depositary receipts and debt, offering different and cost-effective ways to raise capital.
In 2004 the Exchange opened a Hong Kong office and has attracted more than 200 companies from the Asia-Pacific region.
For the biggest companies exists the Premium Listed Main Market.
This operates a Super Equivalence method where conditions of both the UK Listing Authority as well as London Stock Exchange’s own criteria have to be met.
The largest IPO on the Exchange was completed in May 2011 by Glencore International plc.
The company raised $10 billion at admission, making it one of the largest IPOs ever.
In December 2005, the London Stock Exchange rejected a £1.6 billion takeover offer from Macquarie Bank.
The London Stock Exchange described the offer as "derisory", a sentiment echoed by shareholders in the Exchange.
Shortly after Macquarie withdrew its offer, the LSE received an unsolicited approach from NASDAQ valuing the company at £2.4 billion.
This too it rejected.
NASDAQ later pulled its bid, and less than two weeks later on 11 April 2006, struck a deal with LSE's largest shareholder, Ameriprise Financial's Threadneedle Asset Management unit, to acquire all of that firm's stake, consisting of 35.4 million shares, at £11.75 per share.
NASDAQ also purchased 2.69 million additional shares, resulting in a total stake of 15%. While the seller of those shares was undisclosed, it occurred simultaneously with a sale by Scottish Widows of 2.69 million shares.
The move was seen as an effort to force LSE to the negotiating table, as well as to limit the Exchange's strategic flexibility.
Subsequent purchases increased NASDAQ's stake to 25.1%, holding off competing bids for several months.
United Kingdom financial rules required that NASDAQ wait for a period of time before renewing its effort.
On 20 November 2006, within a month or two of the expiration of this period, NASDAQ increased its stake to 28.75% and launched a hostile offer at the minimum permitted bid of £12.43 per share, which was the highest NASDAQ had paid on the open market for its existing shares.
The LSE immediately rejected this bid, stating that it "substantially undervalues" the company.
NASDAQ revised its offer (characterized as an "unsolicited" bid, rather than a "hostile takeover attempt") on 12 December 2006, indicating that it would be able to complete the deal with 50% (plus one share) of LSE's stock, rather than the 90% it had been seeking.
The U.S. exchange did not, however, raise its bid.
Many hedge funds had accumulated large positions within the LSE, and many managers of those funds, as well as Furse, indicated that the bid was still not satisfactory.
NASDAQ's bid was made more difficult because it had described its offer as "final", which, under British bidding rules, restricted their ability to raise its offer except under certain circumstances.
In the end, NASDAQ's offer was roundly rejected by LSE shareholders.
Having received acceptances of only 0.41% of rest of the register by the deadline on 10 February 2007, Nasdaq's offer duly lapsed.
Responding to the news, Chris Gibson-Smith, the LSE's chairman, said: "The Exchange’s strategy has produced outstanding results for shareholders by facilitating a structural shift in volume growth in an increasingly international market at the centre of the world’s equity flows.
The Exchange intends to build on its exceptionally valuable brand by progressing various competitive, collaborative and strategic opportunities, thereby reinforcing its uniquely powerful position in a fast evolving global sector."
On 20 August 2007, NASDAQ announced that it was abandoning its plan to take over the LSE and subsequently look for options to divest its 31% (61.3 million shares) shareholding in the company in light of its failed takeover attempt.
In September 2007, NASDAQ agreed to sell the majority of its shares to Borse Dubai, leaving the United Arab Emirates-based exchange with 28% of the LSE.
In February 2011, in the wake of an announced merger of NYSE Euronext with Deutsche Borse, speculation developed that ICE and Nasdaq could mount a counter-bid of their own for NYSE Euronext.
ICE was thought to be looking to acquire the American exchange's derivatives business, Nasdaq its cash equities business.
As of the time of the speculation, "NYSE Euronext’s market value was $9.75 billion.
Nasdaq was valued at $5.78 billion, while ICE was valued at $9.45 billion.
"Late in the month, Nasdaq was reported to be considering asking either ICE or the Chicago Merc (CME) to join in what would be probably be an $11–12 billion counterbid for NYSE.
On April 1, ICE and Nasdaq made an $11.3 billion offer which was rejected April 10 by NYSE.
Another week later, ICE and Nasdaq sweetened their offer, including a $.17 increase per share to $42.67 and a $350 million breakup fee if the deal were to encounter regulatory trouble.
The two said the offer was a $2 billion (21%) premium over the Deutsche offer and that they had fully committed financing of $3.8 billion from lenders to finance the deal.
The Justice Department, also in April, "initiated an antitrust review of the proposal, which would have brought nearly all U.S. stock listings under a merged Nasdaq-NYSE."
In May, saying it "became clear that we would not be successful in securing regulatory approval," the Nasdaq and ICE withdrew their bid.
The European Commission then blocked the Deutsche merger on 1 February 2012, citing the fact that the merged company would have a near monopoly.
In December 2012, ICE announced it would buy NYSE Euronext for $8.2 billion, pending regulatory approval.
Jeffrey Sprecher will retain his position as Chairman and CEO.
The boards of directors of both ICE and NYSE Euronext approved the acquisition.
In a response to US financial crisis in 2008, Sprecher formed ICE US Trust based in New York, now called ICE Clear Credit LLC, to serve as a limited-purpose bank, a clearing house for credit default swaps.
Sprecher worked closely with the Federal Reserve to serve as its over-the-counter (OTC) derivatives clearing house.
"US regulators were keen on the kind of clearing house for opaque over-the-counter (OTC) derivatives as a risk management device.
In the absence of a central counterparty - which would guarantee pay-outs should a trading party be unable to do so - there was a high risk of massive market disruption(Weitzman 2008)"
The principal backers for ICE US Trust were the same financial institutions most affected by the crisis, the top ten of the world's largest banks (Goldman Sachs, Bank of America, Citi, Credit Suisse, Deutsche Bank, JPMorgan, Merrill Lynch, Morgan Stanley and UBS). Sprecher's clearing house cleared their global credit default swaps (CDS) in exchange for sharing profits with these banks.(Weitzman 2008)(Terhune 2010).
By 30 September 2008 the Financial Post warned that the "$54000bn credit derivatives market faced its biggest test in October 2008 as billions of dollars worth of contracts on now-defaulted derivatives would be auctioned by the International Swaps and Derivatives Association.
In his article in the Financial Post, (Weitzman 2008) described ICE as a "US-based electronic futures exchange" which raised the stakes on October 30, 2008 in its effort to expand in the $54000 bn credit derivatives market.
For years, the NYMEX traders had done a large business trading futures of Maine's potato crop.
According to Leah McGrath Goodman's 2011 book The Asylum, manipulation in this market was commonplace, performed by various parties including potato inspectors and NYMEX traders.
The worst incident was the 1970s potato bust, when Idaho potato magnate J. R. Simplot allegedly went short in huge numbers, leaving a large amount of contracts unsettled at the expiration date, resulting in a large number of defaulted delivery contracts.
A public outcry followed, and the newly created Commodity Futures Trading Commission held hearings.
NYMEX was barred from trading not only potatoes futures, but from entering new areas it hadn't traded in before.
NYMEX's reputation was severely damaged, because, as future chairman Michel Marks told Goodman in his book, "The essence of an exchange is the sanctity of its contract."
When the potato ban came into effect, NYMEX's platinum, palladium and heating oil markets were not significantly affected.
However, NYMEX's reputation suffered in Washington, D.C., especially with the regulations in the Commodity Futures Trading Commission (CFTC), the President of the Exchange, Richard Leone brought in John Elting Treat, White House energy adviser to Presidents Carter and Reagan to help restore the credibility of NYMEX and to help the Exchange explore the possibility of entering the petroleum market recognizing the great potential for moving well beyond the limited size of the New York Heating Oil market.
When Leone left NYMEX in 1981 as a result of a strong disagreement with the NYMEX Board, John Elting Treat was asked to replace him as President.
The launching of the WTI crude oil contract was championed by Treat, who, with difficulty, convinced the Board and the two Marks family members, veteran and highly respected floor trader Francis Marks and his son, Michael, who had just come Chairman of the Board, to take a chance on trading crude oil. Arnold Safir was one of the members of an advisory committee formed by Treat to help design the new contract.
Treat, with Board Chairman Marks and the support of the rest of the NYMEX Board, eventually chose West Texas Intermediate (WTI) as the traded product and Cushing, Oklahoma, as the delivery point.
Robin Woodhead, who later became the first Chairman of the International Petroleum Exchange (IPE) in London started an active dialogue with Treat about whether they could start a Brent Crude oil contracts.
Treat was very supportive and gave Woodhead strong support and a lot of advice. Shortly thereafter, after substantial conversations, The IPE was formally launched and started trading Brent.
NYMEX held a virtual monopoly on "open market" oil futures trading (as opposed to the dark market or "over the counter" market).
However, in the early 2000s the electronically based exchanges started taking away the business of the open outcry markets like NYMEX.
Enron's online energy trading system was part of this trend.
Jeff Sprecher's Intercontinental Exchange, or ICE, was another example.
ICE eventually began trading oil contracts that were extremely similar to NYMEX's, taking away market share almost immediately.
The open outcry NYMEX pit traders had always been against electronic trading because it threatened their income and their lifestyle.
The executives at NYMEX felt that electronic trading was the only way to keep the exchange competitive.
NYMEX teamed up with the Chicago Mercantile Exchange to use Globex in 2006.
The trading pits emptied out as many traders quit. Banks, hedge funds, and huge oil companies stopped making telephone calls to the pits and started trading directly for themselves over screens.
In this period the NYMEX also worked on founding the Dubai Mercantile Exchange in the United Arab Emirates.
This was chronicled by Ben Mezrich in his New York Times Best Selling book Rigged which has been optioned for film adaptation by Summit Entertainment.
The final executive management of NYMEX decided to sell it off in pieces, take golden parachute buyouts, and leave.
In 2006 NYMEX underwent an initial public offering (IPO) and was listed on the New York Stock Exchange.
The executives and exchange members owning seats on the exchange saw their net worth increase by millions of dollars in a few hours - many of the pit traders, who leased their seats instead of owning, did not.
Other parts of NYMEX were sold to private equity investors and the Chicago Mercantile Exchange.
The CME got ownership of the physical facilities and began scrubbing the NYMEX logo and name off of various artifacts and closed the NYMEX museum.
NYMEX eventually became little more than a brand name used by CME.
By 2011, NYMEX open outcry trading was relegated for the most part to a small number of people trading options.
In 2009 it was reported that holders of COMEX gold futures contracts experienced problems taking delivery of their metal.
Along with chronic delivery delays, some investors received delivery of bars not matching their contract in serial number and weight.
The delays could not be easily explained by slow warehouse movements, as the daily reports of these movements showed little activity.
Because of these problems, there were concerns that COMEX did not have the gold inventory to back its existing warehouse receipts.
After launching the original crude oil futures contract, Treat began an aggressive marketing campaign to first bring in the large US and British oil companies and then moved on to pull in the large Middle East producers.
It took almost a year to get the first US "majors" to start trading, but several "majors" did not start for almost 5 years.
The initial resistance from the OPEC producers was almost impossible to break through, although some finally gave in, among the first being Venezuela.
The rumors on the floor at that time were the Arab producers would trade gold futures as a proxy for oil prices (since the Arabs were major purchasers of gold and would buy more when their pockets were filled by rising oil prices, and conversely sell when oil revenues fell and reduced their ability to buy gold).
After the potato ban, NYMEX's existence was in doubt.
Most of the trading volume had been in potatoes.
Without a good commodity, the traders had trouble making money.
Eventually, the new chairman, Michel Marks – the son of commodities icon Francis Q. Marks– along with economist Arnold Safer, figured out that NYMEX could revamp an old heating oil futures contract.
When the government deregulated heating oil, the contract had a chance of becoming a good object of trade on the floor.
A new futures contract was carefully drawn up and trading began on a tiny scale in 1978.
Some of the first users of NYMEX heating oil deliveries were small scale suppliers of people in the Northern United States.
NYMEX's business threatened some entrenched interests like big oil and government groups like OPEC that had traditionally controlled oil prices.
NYMEX provided an "open market" and thus transparent pricing for heating oil, and, eventually, crude oil, gasoline, and natural gas.
NYMEX's oil futures contracts were standardized for the delivery of West Texas Intermediate light, sweet crude oil to Cushing
The energy trading business took off, and NYMEX boomed.
The open outcry floor became a cacophony of shouting traders and pit cards.
The pits became a place where many people without much education or ability to fit into Wall Street could have a chance at being rich.
Many traders and executives became millionaires.
They threw lavish parties and went on exotic vacations.
Many of these people also became heavily involved in drugs and prostitution, with drugs being traded right on the floor of the exchange.
Goodman's book tells the stories of many of the personalities that built the exchange in this era.
COMEX (Commodity Exchange, Inc), one of the exchanges that shared 4 World Trade Center with NYMEX, had traditionally looked down on NYMEX for being smaller and for having the toxic reputation from the potato bust.
With NYMEX's energy trading boom, it became much larger and wealtheir than COMEX.
On August 3, 1994, the NYMEX and COMEX finally merged under the NYMEX name.
By the late 1990s, there were more people working on the NYMEX floor than there was space for them.
In 1997, the NYMEX moved to a new building on the Southwestern portion of Manhattan, part of a complex called the World Financial Center.
The NYBOT's original headquarters and trading floor were destroyed in the September 11, 2001 terrorist attacks on the World Trade Center.
Several NYMEX people were lost in the tragedy, even though the new NYMEX building itself was mostly undamaged.
Despite the area's devastation, the exchange's management and staff had the exchange back up and running in very short order.
On February 26, 2003, the New York Board of Trade (NYBOT) signed a lease agreement with the NYMEX to move into its World Financial Center headquarters and trading facility.
NYMEX maintained a close relationship with many organizations and people at World Trade Center, and After the attacks, the NYMEX built a $12 million trading floor backup facility outside of New York City with 700 traders' booths, 2,000 telephones, and a backup computer system.
This backup was in case of another terrorist attack or natural disaster in Lower Manhattan.
